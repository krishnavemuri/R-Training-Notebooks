{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Green loss calculation\n",
    "This material is originally developed for ACE 592 Big Data - Empirical Economics written on 8/03/2016 by Akito Kamei, Johnathan Rush, and Peter Christensen. The initial material was modifed for a CyberGIS workshop, and more recently has been modified to run outside of the ROGER supercomputer.\n",
    "\n",
    "### Objective:\n",
    "* Read world map (shapefile)\n",
    "* Read green loss satellite file (tiff file)\n",
    "* Plot data\n",
    "\n",
    "### Successful outcome:\n",
    "* Calculate green loss by year\n",
    "\n",
    "#### Information\n",
    "Jupyter Notebooks autosave; check the text next to the file name at the top of the screen to see if it has saved before you close the browser tab. You can also go to the File menu and choose Save and Checkpoint, which gives you a place to revert back to if you need it.\n",
    "\n",
    "If you are done running the notebook, go to File, then Close and Halt to free up system resources for other users. This will clear out all the variables and memory, but any output from the cells you have run will still be displayed. You can run your cells again from the beginning to re-populate variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipak <- function(pkg){\n",
    "new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n",
    "if (length(new.pkg)) \n",
    "    install.packages(new.pkg, dependencies = TRUE)\n",
    "sapply(pkg, require, character.only = TRUE)\n",
    "}\n",
    "\n",
    "# usage\n",
    "packages <- c(\"rgdal\", \"sp\", \"raster\",  \"gdalUtils\", \"foreach\", \"doSNOW\", \"rgeos\")\n",
    "ipak(packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the world map polygon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your current working directory\n",
    "getwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of our current working directory\n",
    "list.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the `basemap` subdirectory\n",
    "list.files(\"basemap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the world map shapefile that is inside of the basemap subdirectory\n",
    "world=readOGR(\"basemap\",layer=\"ne_110m_admin_0_countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the readOGR function do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include a question mark and then a function name to pop up a help window\n",
    "?readOGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print an object, or just enter its name to get some basic information about it.\n",
    "print(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the contents of the shapefile's attribute tabe\n",
    "world@data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also get information on a class of objects:\n",
    "?SpatialPolygonsDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Green loss calculation\n",
    "We want to find out how much green loss there has been by country, by year. It's good to develop your approach with a small data set to start with so you can rapidly test it.\n",
    "We will use the east African country of Guinea Bissau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_Guinea_Bissau <- subset(world, sovereignt==\"Guinea Bissau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_Guinea_Bissau@data[,\"sovereignt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to hold loss year rasters, if it doesn't already exist\n",
    "dir.create(file.path(getwd(), \"lossyear\"), showWarnings = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a raster file with the green loss year encoded for each cell\n",
    "# This one file is 10x10 degrees, and covers all of Guinea Bissau\n",
    "download.file(\"https://storage.googleapis.com/earthenginepartners-hansen/GFC2015/Hansen_GFC2015_lossyear_20N_020W.tif\", \"lossyear/Hansen_GFC2015_lossyear_20N_020W.tif\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Find the raster that overlays Guinea-Bissau\n",
    "# Read year of green loss satellite file (tiff file)\n",
    "lossyear_gb <- raster(\"lossyear/Hansen_GFC2015_lossyear_20N_020W.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the metadata for the raster file we just loaded\n",
    "If you look at a map of Guinea Bissau, you'll see that the country fits entirely within the boundaries expressed by the Corner Coordinates of the raster below.\n",
    "\n",
    "`gdalinfo` needs a filename to operate on, so we are getting the `name` slot of the `file` slot of the RasterLayer object we just made. Of course, we could have given it `\"Hansen_GFC2015_lossyear_20N_020W.tif\"`, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdalinfo(lossyear_gb@file@name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make map of Guinea-Bissau region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with the raster file, which will set the extent of the map\n",
    "plot(lossyear_gb, add=FALSE)\n",
    "plot(world, border=\"red\", lwd = 2, add=TRUE)\n",
    "plot(world_Guinea_Bissau, border=\"black\", lwd = 4, add=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the green cover loss year in Guinea Bissau\n",
    "Often, the approach used for this is the `extract` command in the Raster package. However, the performance of `extract` is poor, and uses an enormous amount of memory to allocate the extracted values.\n",
    "\n",
    "The method below saves computation time by first cropping the raster to the bounding box of the Guinea Bissau polygon, then to the boundaries of the actual polygon, before extracting values. Often, sending smaller input files to a function will speed up the operation, even if many of the values in a large file _should_ be ignored.\n",
    "\n",
    "The ```ext_gb``` object is a vector of every value extracted from the raster. These are then tabulated so that there is a count of every unique value. Finally, the tabulated summary is transformed into a data frame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 <- proc.time() #start timer\n",
    "startTimeGB <- time1\n",
    "clip1_gb <- crop(lossyear_gb, extent(world_Guinea_Bissau)) #crop to extent of polygon\n",
    "cat(\"crop 1:\",\"\\n\"); proc.time() - time1\n",
    "\n",
    "time1 <- proc.time() #start timer\n",
    "clip2_gb <- rasterize(world_Guinea_Bissau, clip1_gb, mask=TRUE) #crops to polygon edge & converts to raster\n",
    "cat(\"crop 2:\",\"\\n\"); proc.time() - time1\n",
    "\n",
    "time1 <- proc.time() #start timer\n",
    "ext_gb <- getValues(clip2_gb) #much faster than extract\n",
    "cat(\"getValues:\",\"\\n\"); proc.time() - time1\n",
    "\n",
    "time1 <- proc.time() #start timer\n",
    "tab_gb <-table(ext_gb) #tabulates the values of the raster in the polygon\n",
    "cat(\"tabulate:\",\"\\n\"); proc.time() - time1\n",
    "\n",
    "time1 <- proc.time() #start timer\n",
    "mat_gb <- as.data.frame(tab_gb)\n",
    "cat(\"as.data.frame(tab):\",\"\\n\"); proc.time() - time1\n",
    "\n",
    "timeGB <- proc.time() - startTimeGB\n",
    "cat(\"Total time to extract raster cell values in Guinea Bissau:\\n\")\n",
    "print(summary(timeGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at the summarized results data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the summarized data frame as a bar chart\n",
    "\n",
    "##### How to make a bar plot?\n",
    "We'll check the help in the cell below. There, we've used the :: symbol to indicate which specific library's barplot function we want to use.\n",
    "\n",
    "The `raster` library overrides and extends the built-in `barplot` function from the `graphics` library. So if we just ran `?barplot`, we'd get information on using `raster`'s version. However, there is better information on the basic capabilities of the barplot function in the `graphics` library's help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?graphics::barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using values from position 2 on will exclude the first row - the 0 code for no loss.\n",
    "# The 0 code is much more common than any other, and would make the plot hard to read.\n",
    "GBlossyears <- mat_gb$Freq[2:length(mat_gb$Freq)]\n",
    "GBlosscodes <- mat_gb$ext_gb[2:length(mat_gb$ext_gb)]\n",
    "#Label setting\n",
    "GBlosscodes <- factor(GBlosscodes,\n",
    "levels = c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\"),\n",
    "labels = c(\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\"))\n",
    "\n",
    "barplot(height = GBlossyears, names.arg = GBlosscodes, xlab = \"Code for Year of Green Loss\", beside=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 4: Larger regions, Memory Management, and Parallelization\n",
    "\n",
    "# Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a copy of Indonesia's boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_Indonesia <- subset(world, sovereignt==\"Indonesia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Indonesia's green loss rasters\n",
    "These nine files are about 290 MB all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector of the files covering Indonesia\n",
    "ind_files <- c(\"Hansen_GFC2015_lossyear_00N_090E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_00N_100E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_00N_110E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_00N_120E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_10N_090E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_10N_100E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_10N_110E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_10N_120E.tif\", \n",
    "                 \"Hansen_GFC2015_lossyear_10S_120E.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localpath <- \"lossyear\"\n",
    "remotepath <- \"https://storage.googleapis.com/earthenginepartners-hansen/GFC2015\"\n",
    "for(i in 1:length(ind_files)){\n",
    "    file <- ind_files[i]\n",
    "    cat(\"Downloading \", file, \"\\n\")\n",
    "    download.file(paste(remotepath, file, sep=\"/\"), paste(localpath, file, sep=\"/\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a *virtual raster* for ```lossyear``` files in for the Indonesia region\n",
    "One raster file covered the entire region of Guinea Bissau, but we will need to use several adjacent rasters to cover Indonesia.\n",
    "\n",
    "With a virtual raster file, we can reference many files as a single object. Virtual rasters have the file extension ```.vrt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setwd(\"/mnt/jhub/lossyear\")\n",
    "#Create a vector of file names for the Indonesia area raster files\n",
    "ind_lossyear <- c(\"lossyear/Hansen_GFC2015_lossyear_00N_090E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_00N_100E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_00N_110E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_00N_120E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_10N_090E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_10N_100E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_10N_110E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_10N_120E.tif\", \n",
    "                 \"lossyear/Hansen_GFC2015_lossyear_10S_120E.tif\")\n",
    "\n",
    "# create a VRT out of the input files, store in your directory\n",
    "vrtpath <- paste(getwd(), \"indonesia.vrt\", sep=\"/\")\n",
    "gdalbuildvrt(ind_lossyear, vrtpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a virtual raster that refers to the TIFF files covering Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndRasters <- raster(vrtpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways of getting information on the rasters we've loaded:\n",
    "print(IndRasters) #print the default information for the raster object\n",
    "print(gdalinfo(IndRasters@file@name)) #print the results of gdalinfo on the filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss rasters with the national boundaries on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 <- proc.time() #start timer\n",
    "\n",
    "plot(IndRasters, add=FALSE)\n",
    "plot(world, border=\"red\", lwd = 1, add=TRUE)\n",
    "plot(world_Indonesia, border=\"black\", lwd = 2, add=TRUE)\n",
    "\n",
    "time2 <- proc.time() #end timer\n",
    "cat(\"Plotting time for Indonesia:\",\"\\n\")\n",
    "summary(time2 - time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting raster values from Indonesia\n",
    "We established earlier that extracting values from Indonesia the usual way would either take too long, or use too much memory.\n",
    "\n",
    "One data parallelism strategy is called \"chunking.\" Breaking the operations into smaller chunks can let you use more processors at once, as well as better manage how much memory you use.\n",
    "\n",
    "Below, we will divide Indonesia into smaller pieces and summarize the extracted values from each one. By keeping only what we really want for each chunk - the summary of unique values - we can discard the array of every pixel value from each chunk as soon as we're done summarizing it. This will reduce the amount of memory we need to use at any one time.\n",
    "\n",
    "After chunking, we will learn how to process many of the chunks at once to speed up computation.\n",
    "\n",
    "#### How many chunks?\n",
    "There is a little overhead in terms of storage and communication for every chunk you make, so you don't want to split up the source data _too_ much. One way to estimate if we are hitting the right balance is to take a look at the plotted map after we have the chunks. Better methods include checking the number of pixels in a chunk, the memory used by the values extracted for a chunk, or the ratio of a chunk's area to the area of a region we have already calculated (like Guinea Bissau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 <- proc.time() #start timer\n",
    "\n",
    "# Get extent for country\n",
    "ind_yext <- world_Indonesia@bbox[\"y\",\"max\"] - world_Indonesia@bbox[\"y\",\"min\"]\n",
    "ind_xext <- world_Indonesia@bbox[\"x\",\"max\"] - world_Indonesia@bbox[\"x\",\"min\"]\n",
    "cat(\"Indonesia longitude extent: \", ind_xext, \"; latitude extent: \", ind_yext)\n",
    "\n",
    "# Divide lat/long extents by U/V increments to create new polygons\n",
    "# Here, we've manually chosen divisions, but you can imagine setting a maximum width for these\n",
    "udiv = 45; vdiv = 15;\n",
    "\n",
    "# Extent in map units for each rectangle\n",
    "ind_yint <- ind_yext / vdiv\n",
    "ind_xint <- ind_xext / udiv\n",
    "\n",
    "# What is the size of each rectangle?\n",
    "cat(\"\\nSubdivision rectangle width:  \", ind_xint)\n",
    "cat(\"\\nSubdivision rectangle height: \", ind_yint)\n",
    "\n",
    "#initialize an empty list with the length to hold all the grid cells we will create\n",
    "polys <- vector(\"list\", udiv*vdiv) #make a vector of type list, with a length of udiv * vdiv\n",
    "\n",
    "#loop\n",
    "for (u in 1:udiv){\n",
    "    for (v in 1:vdiv){     \n",
    "        #store the calculated extents as polygons in an indexed array\n",
    "        polys[[(u-1)*vdiv+v]] <- as(extent(world_Indonesia@bbox[\"x\",\"min\"] + ind_xint * (u-1),\n",
    "                                           world_Indonesia@bbox[\"x\",\"min\"] + ind_xint * u, \n",
    "                                           world_Indonesia@bbox[\"y\",\"min\"] + ind_yint * (v-1),\n",
    "                                           world_Indonesia@bbox[\"y\",\"min\"] + ind_yint * v),\n",
    "                                     'SpatialPolygons')\n",
    "    }\n",
    "}\n",
    "\n",
    "#merge polygons - this calls the bind function on each polygon to make them a single object\n",
    "merged_polys <- do.call(bind, polys) \n",
    "\n",
    "#set the projection of these new shapes to the same as is used for our Indonesia shape\n",
    "projection(merged_polys) <- projection(world_Indonesia)\n",
    "\n",
    "cat(\"\\nSubdivision rectangle creation time\",\"\\n\"); proc.time() - time1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the newly created cells to check that they look correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(merged_polys, col='red', axes=TRUE)\n",
    "plot(world_Indonesia, col=rgb(1,1,1,0.5), add=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersected result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Intersected chunks with country shape\n",
    "chunks_Ind <- intersect(world_Indonesia, merged_polys)\n",
    "plot(chunks_Ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many cells overlapped with Indonesia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(chunks_Ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract values from Indonesia chunks - Serial version\n",
    "We will extract the loss year values from small regions of Indonesia, one at a time. This solves the problem of loading too much data into memory.  \n",
    "However, there are enough chunks that this would take a long time. We'll stop early, and you can extrapolate out how long it would take to extract the values for the whole country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapecount <- length(chunks_Ind) # total number of chunks\n",
    "\n",
    "endcount <- 10 # choose a number less than shapecount for testing\n",
    "mat <- vector(\"list\", endcount)\n",
    "\n",
    "extractor <- function(p){\n",
    "    clip1_Ind <- crop(IndRasters, extent(chunks_Ind[p,])) #crop to extent of polygon\n",
    "    clip2_Ind <- rasterize(chunks_Ind[p,], clip1_Ind, mask=TRUE)\n",
    "    ext <- getValues(clip2_Ind) #much faster than raster::extract\n",
    "    print(ext)\n",
    "    tab<-table(ext) #tabulates the values of the raster in the polygon\n",
    "    print(tab)\n",
    "}\n",
    "\n",
    "starttimeInd <- proc.time() #begin time\n",
    "\n",
    "#apply (run) the extractor function for each chunk of Indonesia, up to the endcount limit\n",
    "tablesInd <- lapply(1:endcount, function(p) extractor(p)) \n",
    "    \n",
    "dfInd <- do.call(cbind, tablesInd) #call the cbind (combine by columns) function on each table in the `tablesInd` list\n",
    "endtimeInd <- proc.time() #end time\n",
    "\n",
    "cat(\"Time to extract values from \", endcount, \" polygons:\\n\")\n",
    "print(summary(endtimeInd - starttimeInd))\n",
    "\n",
    "cat(\"\\nEstimated total time for serial extraction of all Indonesia values:\\n\")\n",
    "cat((endtimeInd - starttimeInd)[[3]] * length(chunks_Ind) / endcount / 60, \" minutes\")\n",
    "\n",
    "#dfInd is a data frame with a column for every table from the tablesInd list.\n",
    "# Sum across each row to get the frequency of each raster value\n",
    "sumInd <- as.data.frame(rowSums(dfInd)) \n",
    "colnames(sumInd) <- \"Freq\" #assign the name Freq to the column in the data frame to be consistent with Guinea Bissau method\n",
    "sumInd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract values from Indonesia chunks - Parallel version\n",
    "As above, but using `foreach` instead of `lapply`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapecount <- length(chunks_Ind) # total number of chunks\n",
    "\n",
    "endcount <- 10 # use the same number as the above serial version to measure speedup\n",
    "              # SET TO SHAPECOUNT to extract data for all of Indonesia\n",
    "extractor <- function(p){1\n",
    "    clip1_Ind <- crop(IndRasters, extent(chunks_Ind[p,])) #crop to extent of polygon\n",
    "    clip2_Ind <- rasterize(chunks_Ind[p,], clip1_Ind, mask=TRUE)\n",
    "    ext <- getValues(clip2_Ind) #much faster than raster::extract\n",
    "    tab<-table(ext) #tabulates the values of the raster in the polygon\n",
    "}\n",
    "\n",
    "# Change `4` to however many processors your system has\n",
    "cluster = makeCluster(4, type = \"SOCK\")\n",
    "registerDoSNOW(cluster)\n",
    "clusterExport(cluster, c(\"IndRasters\",\"chunks_Ind\", \"extractor\"))\n",
    "ptm <- proc.time()    \n",
    "results = foreach(n = 1:endcount, .combine = cbind) %dopar% {\n",
    "     library(raster); \n",
    "     #Define function\n",
    "     extractor(n)\n",
    "}\n",
    "#results\n",
    "proc.time() - ptm \n",
    "stopCluster(cluster)\n",
    "\n",
    "sumInd <- as.data.frame(rowSums(results))\n",
    "colnames(sumInd) <- \"Freq\"\n",
    "sumInd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How much did the parallel code speed up a sequential code?\n",
    "Sp = Ts / Tp\n",
    "- p = # of processors\n",
    "- Ts = execution time of the sequential code\n",
    "- Tp = execution time of the parallel code with p processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "167.559 / 63.288"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
